#!/usr/bin/env python3
"""
Quick Resource Reference Card
A simple reference for your workflow resource configurations
"""

def print_resource_card():
    """Print a quick resource reference card"""
    
    card = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    ğŸš€ ML WORKFLOW RESOURCE CARD                      â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                      â•‘
â•‘  ğŸ“‹ load_data        â”‚ 500m CPU, 500Mi RAM  â”‚ max: 1 CPU, 1Gi RAM   â•‘
â•‘  ğŸ“‹ clean_data       â”‚ 200m CPU, 200Mi RAM  â”‚ max: 500m CPU, 500Mi   â•‘
â•‘  ğŸ“‹ features         â”‚ 300m CPU, 300Mi RAM  â”‚ max: 500m CPU, 500Mi   â•‘
â•‘  ğŸ“‹ train_model      â”‚ 800m CPU, 800Mi RAM  â”‚ max: 1 CPU, 1Gi RAM    â•‘
â•‘  ğŸ“‹ evaluate_model   â”‚ 200m CPU, 200Mi RAM  â”‚ max: 500m CPU, 500Mi   â•‘
â•‘                                                                      â•‘
â•‘  ğŸ“Š Total Requests: 2.0 CPU, 2000Mi RAM (~2Gi)                      â•‘
â•‘  ğŸš¨ Total Limits:   3.5 CPU, 3500Mi RAM (~3.5Gi)                    â•‘
â•‘                                                                      â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  ğŸ’¡ Quick Reference:                                                 â•‘
â•‘     â€¢ 1000m = 1 CPU core                                             â•‘
â•‘     â€¢ 1024Mi = 1Gi (approximately)                                   â•‘
â•‘     â€¢ Requests = guaranteed minimum                                  â•‘
â•‘     â€¢ Limits = maximum allowed                                       â•‘
â•‘                                                                      â•‘
â•‘  ğŸ”§ To modify resources, edit ml_pipeline_improved.py:              â•‘
â•‘     @task(requests=Resources(cpu="500m", mem="500Mi"))              â•‘
â•‘                                                                      â•‘
â•‘  ğŸ“ˆ Monitoring:                                                      â•‘
â•‘     â€¢ UI: Individual task details (JSON format)                     â•‘
â•‘     â€¢ CLI: python resource_summary.py                               â•‘
â•‘     â€¢ Live: python live_execution_monitor.py <execution-id>         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
    print(card)
    
    # Calculate some useful stats
    total_requests_cpu = 500 + 200 + 300 + 800 + 200  # millicores
    total_requests_mem = 500 + 200 + 300 + 800 + 200  # Mi
    
    total_limits_cpu = 1000 + 500 + 500 + 1000 + 500  # millicores
    total_limits_mem = 1024 + 500 + 500 + 1024 + 500  # Mi (1Gi = 1024Mi)
    
    print(f"ğŸ“ˆ Resource Analysis:")
    print(f"   â€¢ Peak CPU usage: {total_requests_cpu}m ({total_requests_cpu/1000:.1f} cores)")
    print(f"   â€¢ Peak Memory usage: {total_requests_mem}Mi ({total_requests_mem/1024:.1f}Gi)")
    print(f"   â€¢ Max possible CPU: {total_limits_cpu}m ({total_limits_cpu/1000:.1f} cores)")
    print(f"   â€¢ Max possible Memory: {total_limits_mem}Mi ({total_limits_mem/1024:.1f}Gi)")
    print()
    print(f"ğŸ¯ Cluster Requirements:")
    print(f"   â€¢ Minimum node size: {total_requests_cpu/1000:.1f} CPU, {total_requests_mem/1024:.1f}Gi RAM")
    print(f"   â€¢ Recommended node size: {total_limits_cpu/1000:.1f} CPU, {total_limits_mem/1024:.1f}Gi RAM")


if __name__ == "__main__":
    print_resource_card()
